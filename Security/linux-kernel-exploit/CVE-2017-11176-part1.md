# CVE-2017-11176 part1
[LEXFO](https://blog.lexfo.fr/) 홈페이지에 게시되어있는 CVE-2017-11176 관련 문서를 번역하며 공부했습니다.

# 핵심개념
CVE 분석에서 길을 잃지 않기 위해서는 Linux 커널의 핵심 개념들에 대해 이해해야한다. Part1에서는 exploit에 필요한 커널 개념에 대해서 소개한다.

## task_struct

커널에서 가장 중요한 구조 중 하나가 `struct task_struct` 이다. 모든 작업에는 메모리에 있는 `task_struct` 객체가 있다. 유저영역에서 돌아가는 프로세스에는 적어도 한개의 task_struct가 존재하고, multi thread 프로그램의 경우 프로세스 개수는 영향이 없으므로 모든 스레드에 대해 하나의 `task_struct`가 있다. 커널 스레드의 경우, 자체 `task_struct(kworker, migration)` 이 존재한다.

`task_struct` 는 다음과 같은 정보를 포함한다.

```cpp
struct task_struct {
    volatile long state;            // process state (running, stopped, ...)
    void *stack;                    // task's stack pointer
    int prio;                       // process priority
    struct mm_struct *mm;           // memory address space
    struct files_struct *files;     // open file information
    const struct cred *cred;        // credentials
    // ...
};
```

프로세스에 대한 다양한 주요 데이터를 포함하고 있지만, 본 글에서는 files_struct 구조체로 선언된 files에 집중해보려 한다.

## File Descriptor, File Object and File Descriptor Table

리눅스에서 흔히 `"모든 것을 파일로 관리한다"` 라고 설명하곤 한다. 그렇다면 이는 실제로 무엇을 의미하는 것일까?

리눅스 커널에는 기본적으로 일반, 디렉토리, 링크, 캐릭터 디바이스, 블록 디바이스, FIFO 그리고 소켓의 7가지 종류의 파일이 있다. 각각 `파일 디스크립터`로 표현될 수 있으며, 이러한 `파일 디스크립터` 는 기본적으로 고유한 정수값으로 이루어져있다.

`file 구조체` 또는 `file 객체`는 디스크립터를 통해 파일에 접근하게 되면 생성되는 구조체로, 열려있는 파일을 표현한다.  `file 구조체` 에 가장 중요한 필드들은 다음과 같다.

```cpp
struct file {
    loff_t                          f_pos;          // "cursor" while reading file
    atomic_long_t                   f_count;        // object's reference counter
    const struct file_operations    *f_op;          // virtual function table pointer
    void                            *private_data;  // used by file "specialization"
    // ...
};
```

  정수형으로 표현되는 파일 디스크립터를 통해 file 구조체 포인터로 변환하기 위해서는 일종의 매핑작업이 필요한데, `file descriptor table(FDT)`가 담당한다. 이는 1:1 매핑이 아니며, 몇몇 파일 디스크립터가 동일한 file 객체를 가리킬 수 있다. 이러한 경우, 가리켜진 file 객체는 `reference counter(f_count 필드)`가 1 증가한다. FDT는 `fdtable 구조체`에 저장된다. 이것은 그저 파일 디스크립터로 인덱싱이 가능한 file 구조체 포인터의 배열이다.

```cpp
struct fdtable {
    unsigned int max_fds;
    struct file ** fd;      /* current fd array */
    // ...
};
```

fdtable이 배열 형태로 사용되는걸 직접 확인해야 편-안할 듯.

  FDT를 프로세스에 연결하는 것은 `files_struct` 구조체다. 즉, FDT를 참조하기 위해 files_struct를 먼저 참조해야 한다.  `files_struct` 구조체는 여러 스레드 사이에서 공유 될 수 있다.

```cpp
struct files_struct {
    atomic_t count;           // reference counter
    struct fdtable *fdt;      // pointer to the file descriptor table
    // ...
};
```

  `files_struct` 에 대한 포인터는 `task_struct` 의 files 필드에 저장된다.

## Virtual Function Table(VFT)

  리눅스는 대부분 C로 구현되지만, 여전히 객체지향 커널이다. 객체지향성을 갖추기 위한 한 가지 방법은 `virtual function table(VFT)` 를 사용하는 것이다. VFT는 대부분 함수 포인터로 구성되어있는 구조이다.

가장 잘 알려진 VFT는 `file_operations` 구조체다.

```cpp
struct file_operations {
    ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
    ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
    int (*open) (struct inode *, struct file *);
    int (*release) (struct inode *, struct file *);
  // ...
};
```

모든 것들이 파일로 이루어져있지만, 리눅스가 애초에 모든 것들을 파일로 관리하기 때문에, 파일이라고 해서 다 같은 타입이 아니다. 그렇기 때문에 각 파일이 모두 f_ops라고 하는 각기 다른 file operations를 가지고 있다. 이렇게 하면 커널 코드가 각각의 파일을 독립적으로 처리할 수 있다.

예를 들면 다음과 같은 흐름으로 이어진다.

```cpp
if (file->f_op->read)
	ret = file->f_op->read(file, buf, count, pos);
```

file 구조체의 file_operations (f_op 필드)를 참조하여 read 함수를 실행시킨다. 이는 커널은 같은 read 함수를 실행시키지만 파일마다 다른 동작이 가능해진다.

## Socket, Sock and SKB

`socket` 구조체는 network stack에서 최상단에 위치해있다. 소켓이 생성(socket syscall)되면, 새 file 구조체가 생성되고, 이것의 file operation(f_op 필드)은 `socket_file_ops`로 설정된다. 즉, 새로운 파일이 하나 생성된 것이며, 당연하게도 파일 디스크립터 또한 존재한다.

  모든 파일이 파일 디스크립터로 표현되기 때문에, 소켓 파일 디스크립터와 함께 파일 디스크립터를 인자로 사용하는 모든 syscall(e.g. read(), write(), close())을 사용할 수 있다. 이것이 `"모든 것을 파일로 관리한다"` 의 모토가 되는 주요한 이점이다. 소켓의 종류와는 관계없이, 커널은 일반 소켓 file operation을 호출한다.

```cpp
static const struct file_operations socket_file_ops = {
    .read = sock_aio_read,      // <---- calls sock->ops->recvmsg()
    .write =    sock_aio_write, // <---- calls sock->ops->sendmsg()
    .llseek =   no_llseek,      // <---- returns an error
  // ...
}
```

  소켓 구조체는 BSD socket API(connect(), bind(), accept(), listen(), ... )를 구현하기 위해, `proto_ops 구조체`라는 특별한 VFT를 포함한다.

```cpp
struct proto_ops {
    int     (*bind)    (struct socket *sock, struct sockaddr *myaddr, int sockaddr_len);
    int     (*connect) (struct socket *sock, struct sockaddr *vaddr,  int sockaddr_len, int flags);
    int     (*accept)  (struct socket *sock, struct socket *newsock, int flags);
  // ...
}
```

  BSD 스타일의 syscall(e.g. bind())이 호출되었을 때, 커널은 일반적으로 다음과 같은 체계를 따른다.

1. file 구조체를 FDT에서 찾는다
2. file 구조체에서 socket 구조체를 찾는다
3. specialized된 `proto_ops 콜백`(e.g. sock→sps→bind())을 호출한다

file이 socket을 어떻게 찾지?

  일부 프로토콜 작업(e.g. 송신/수신 데이터)이 실제로 네트워크 스택의 로우한 layer에 들어가는 것을 필요로 하기 때문에, `socket 구조체` 는 `sock 구조체`의 객체에 대한 포인터를 가지고 있다. 이 포인터는 일반적으로 소켓 프로토콜 작업(proto_ops)에 의해 사용된다. 결국, `socket 구조체`는 `file 구조체`와 `sock 구조체`사이의 일종의 접착제이다.

```cpp
struct socket {
    struct file     *file;
    struct sock     *sk;
    const struct proto_ops  *ops;
  // ...
};
```

sock 구조체는 복잡한 데이터 구조다. network card driver와 같이 low한 계층과와 socket과 같은 상위계층 사이의 중간 계층으로 볼 수 있다. 주요한 목적은 일반적인 방법으로 송/수신 버퍼를 유지하는 것이다.

network card를 통해 패킷이 수신되면, 드라이버는 패킷을 소켓 수신 버퍼(큐)에 삽입(enqueued)한다. 이렇게 삽입된 패킷은 프로그램이 수신하겠다고 결정(recvmsg syscall)할때까지는 계속 버퍼에 남아있게 된다. 반대로 프로그램이 데이터를 보내려고 할 때(sendmsg syscall)도, 해당 패킷은 소켓 송신 큐에 삽입된다. 전송 및 사용 이벤트가 발생하면, 큐에서 패킷을 빼서(dequeued) 전송 또는 사용한다.

이러한 버퍼를 sk_buff 또는 skb 구조체라고 한다. 송/수신 버퍼는 기본적으로 skb의 더블 링크드리스트 형태로 구성되어있다.

```cpp
struct sock {
    int         sk_rcvbuf;    // theorical "max" size of the receive buffer
    int         sk_sndbuf;    // theorical "max" size of the send buffer
    atomic_t        sk_rmem_alloc;  // "current" size of the receive buffer
    atomic_t        sk_wmem_alloc;  // "current" size of the send buffer
    struct sk_buff_head sk_receive_queue;   // head of doubly-linked list
    struct sk_buff_head sk_write_queue;     // head of doubly-linked list
    struct socket       *sk_socket;
    // ...
}
```

위에서 볼 수 있듯이, sock 구조체를 보면 socket 구조체(sk_socket 필드)를 포함하는 것을 알 수 있다. 위로 조금 올려보면 socket 구조체를 볼 수 있는데 또다시 sock 구조체(sk필드)를 포함하고 있음을 알 수 있다.

같은 방법으로, socket 구조체는 file 구조체(file 필드)를 참조하는 반면, file구조체는 socket구조체(private_data 필드)를 참조한다.

이러한 구조(2-way 메커니즘)로 데이터는 네트워크 스택을 통해 위-아래로 이동할 수 있다.

## Netlink Socket

netlink socket은 앞서 설명했던 소켓들의 일종이다. 이 소켓(AF_NETLINK)은 다양한 동작이 가능한데, 해당되는 내용은 아래와 같다.

- **NETLINK_ROUTE**
라우팅 테이블 수정
- **NETLINK_SELINUX**
SELinux 이벤트 알림
- **NETLINK_USERSOCK**
다른 유저영역 프로세스들과 통신

이렇듯 sock 구조체와 socket 구조체는 다양한 소켓 통신을 지원하기 위해 가장 기반이 되는 구조이기 때문에, specialize 해야한다.

socket 구조체의 관점에서, proto_ops 필드는 정의되어야 한다. Netlink에서는 이러한 socket operation가 netlink_ops로 구현되어있다.

```cpp
static const struct proto_ops netlink_ops = {
    .bind =     netlink_bind,
    .accept =   sock_no_accept,     // <--- calling accept() on netlink sockets leads to EOPNOTSUPP error
    .sendmsg =  netlink_sendmsg,
    .recvmsg =  netlink_recvmsg,
    // ...
}
```

sock 구조체의 관점에서 보면, 약간 더 복잡해진다. sock 구조체는 추상 클래스로 볼 수 있다. 따라서 sock 구조체는 specialized 되어야 한다. netlink의 경우 이는 netlink_sock으로 구현되어있다.

```cpp
struct netlink_sock {
    /* struct sock has to be the first member of netlink_sock */
    struct sock     sk;
    u32         pid;
    u32         dst_pid;
    u32         dst_group;
    // ...
};
```

다른말로, netlink_sock 구조체는 여러가지 속성들이 추가된 sock이라고 볼 수 있다. 그저 netlink의 사용 용도에 따라 sock을 상속받아 필요한 필드들을 추가했다고 보면 될 것 같다. 

최상위 수준에서의 논평은 매우 중요하다. 커널이 정확한 type을 모르는 상태에서 일반적인 sock 구조체를 다룰 수 있게 되는데, 일반적으로 C언어는 이러한 다형성에 대한 특징을 가지고있는 언어가 아니다. 하지만, 위와 같은 방법을 통해 netlink_sock.sk와 netlink_sock을 구현했고, 이를 해제하면 해당 객체 및 sock 구조체도 해제된며, 최종적으로 이러한 구조는 다형성이라는 이점을 가져다준다. netlink_sock 코드의 life cycle logic은 일반적이고, 잘 테스트된 코드이다.

## Putting it all together

앞서 설명한 내용이 커널 exploit을 연구하는데에 있어 가장 기초적이고 필수적인 개념이다. 이를 종합하여 하나의 다이어그램으로 시각화하면 다음과 같다.

![core_struct_relationship](/.resource/core_struct_relationship.png)

각 화살표는 포인터를 나타내며, 서로 교차하는 선은 없다. 

## Reference counters

커널의 core부분의 개념정리를 마무리하기 위해서는 리눅스 커널이 reference counters를 어떻게 처리하는에 대해 이해하는 것이 필요하다.

커널에서의 memory leak을 감소시키고, UAF를 예방하기 위해 대부분의 리눅스에서는 data 구조에 ref counter라는것이 내장되어있다. refcounter는 정수형인 atomic_t 자료형으로 표현되며, 값을 변경하는 것은 다음과 같은 방법으로만 가능하다.

- atomic_inc()
- atomic_add()
- atomic_dec_and_test()  // 1 감소 후 0인지 확인

스마트 포인터나 연산자 오버로드 방식이 없기 때문에, reference counter는 개발자에 의해 수동으로 처리해야한다. 즉, 다른 object에 의해 참조될 때, reference counter를 명시적으로 늘려주어야 하며, 이것이 0이 되었을 때, 해당 객체는 참조로부터 자유롭다고 할 수 있다.

> ref counter를 증가시키는 것을 "taking a reference"라고 표현하며, 감소시키는 것을 "dropping/releasing a reference"라고 한다.

리눅스 커널은 ref counters를 다룰 수 있는 몇가지 기능(kref, kobject)들이 있다. 그러나 체계적으로 사용되지 않으며, 여기서 다룰 객체들은 그들만의 고유한 reference counter helper를 가지고 있다. 일반적으로, 대부분 **"*_get()"**이나 **"*_put()"**과 같이 명명된다.

> 혼란이 야기될 만한 부분이 있다. skb_put()은 "*_put()"의 형태를 띄지만 어떠한 refcounter도 줄이지 않는다. 그저 sk buffer에 "push"할 뿐이다.

이렇게 버그를 이해하기 위해 필요한 모든 data 구조들을 알아보았다. 이제 CVE 분석을 시작하면 된다.


## Reference
- https://askubuntu.com/questions/418666/update-grub-command-not-found  
- https://dev-ahn.tistory.com/96  
- https://linuxholic.tistory.com/entry/리눅스-Taskstruct-구조  
- https://proneer.tistory.com/entry/Netlink-Socket